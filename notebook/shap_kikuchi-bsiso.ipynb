{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/gi55/i55233/miniconda3/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow\n",
    "from numpy.linalg.linalg import norm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import activations\n",
    "#from tf_keras_vis.activation_maximization import ActivationMaximization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, LayerNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.markers import MarkerStyle\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import pandas as pd\n",
    "import shap\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "# 画像用\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "# モデル読み込み用\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data =  ['lat', 'lon', 'time', 'real_time', 'olr', 'u850', 'v850', 'u200', 'v200', 'h850', 'pr_wtr', 'sst']\n",
      "(25,) (144,) (15991, 25, 144) (15991, 25, 144) (15991, 25, 144) (15991, 25, 144) (15991, 25, 144) (15991, 25, 144) (15991, 25, 144)\n",
      "1979-03-22 00:00:00 2022-12-31 00:00:00\n",
      "Raw Data        =  110.6346296477856 -163.06319252604578\n",
      "Normalized Data =  8.610244733289218 -15.696228824469003\n",
      "Raw Data        =  26.628822697430984 -27.39433673019417\n",
      "Normalized Data =  8.718597179911693 -6.659368241725141\n",
      "Raw Data        =  19.607371330660023 -21.027599307874095\n",
      "Normalized Data =  9.371160190483371 -9.169251236677564\n",
      "Raw Data        =  54.12790452689419 -59.46183253683652\n",
      "Normalized Data =  6.598683947935241 -8.180679755615591\n",
      "Raw Data        =  50.62647740730578 -45.80567673624702\n",
      "Normalized Data =  7.36514770028667 -6.98696916292694\n",
      "Raw Data        =  132.37072580688695 -196.37969980408127\n",
      "Normalized Data =  5.4903753314082575 -9.535849081936803\n",
      "Raw Data        =  36.488246148282585 -32.8364699960708\n",
      "Normalized Data =  8.03784897444771 -6.679418853230725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3488056/527272249.py:24: RuntimeWarning: invalid value encountered in divide\n",
      "  data_norm = (data - data_mean) / data_std\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data        =  4.588571921210745 -4.560916137984673\n",
      "Normalized Data =  nan nan\n",
      "PCs =  (15981, 2)\n",
      "time PCs=  (15981,)\n",
      "real time PCs =  1979-04-01 00:00:00 2022-12-31 00:00:00\n",
      "==== lead time : 0 day =====\n",
      "(15981, 2)\n",
      "(2556, 2) (13424, 2)\n",
      "(2556, 25, 144, 24)\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "data = np.load('/work/gi55/i55233/data/results/bsiso_eeof/prepro_anomaly_8vals.npz')\n",
    "print('data = ', data.files)\n",
    "\n",
    "lat = data['lat'][24:49]\n",
    "lon = data['lon']\n",
    "olr = data['olr'][80:,24:49,:]\n",
    "u850 = data['u850'][80:,24:49,:]\n",
    "v850 = data['v850'][80:,24:49,:]\n",
    "u200 = data['u200'][80:,24:49,:]\n",
    "v200 = data['v200'][80:,24:49,:]\n",
    "h850 = data['h850'][80:,24:49,:]\n",
    "pr_wtr = data['pr_wtr'][80:,24:49,:]\n",
    "sst = data['sst'][80:,24:49,:]\n",
    "time = data['time'][80:]    # 射影後にデータが10日進むため、時刻の方を前進させておく\n",
    "real_time = pd.to_datetime(time, unit='h', origin=pd.Timestamp('1800-01-01')) # 時刻をdatetime型に変換\n",
    "print(lat.shape, lon.shape, olr.shape, u850.shape, v850.shape, u200.shape, v200.shape, h850.shape, pr_wtr.shape)\n",
    "print(real_time[0], real_time[-1])\n",
    "\n",
    "# 標準化処理\n",
    "def normalization(data):\n",
    "  data_mean = np.mean(data, axis=0)\n",
    "  data_std  = np.std(data, axis=0)\n",
    "  data_norm = (data - data_mean) / data_std\n",
    "  print('Raw Data        = ', data.max(), data.min())\n",
    "  print('Normalized Data = ', data_norm.max(), data_norm.min())\n",
    "  data_norm = np.nan_to_num(data_norm, nan=0) # 欠損値(nan)を0で置換\n",
    "  del data_mean, data_std\n",
    "  return data_norm\n",
    "\n",
    "olr_norm  = normalization(olr)\n",
    "u850_norm = normalization(u850)\n",
    "v850_norm = normalization(v850)\n",
    "u200_norm = normalization(u200)\n",
    "v200_norm = normalization(v200)\n",
    "h850_norm = normalization(h850)\n",
    "pr_wtr_norm = normalization(pr_wtr)\n",
    "sst_norm = normalization(sst)\n",
    "\n",
    "# bsiso index (eEOF) 読み込み\n",
    "data_file = '/work/gi55/i55233/data/results/bsiso_eeof/bsiso_rt-PCs.npz'\n",
    "PC      = np.load(data_file)['rt_PCs'][:,:2]\n",
    "sign    = np.array([-1, 1]).T\n",
    "PC_norm = sign * PC / PC.std(axis=0)[np.newaxis,:]\n",
    "time2   = np.load(data_file)['time']\n",
    "real_time2 = pd.to_datetime(time2, unit='h', origin=pd.Timestamp('1800-01-01')) # 時刻をdatetime型に変換\n",
    "\n",
    "print('PCs = ', PC_norm.shape)\n",
    "print('time PCs= ', time2.shape)\n",
    "print('real time PCs = ', real_time2[0], real_time2[-1])\n",
    "\n",
    "# インデクシングする関数\n",
    "def indexing(lead_time):\n",
    "  output_shape = 2\n",
    "  rt = real_time2[:-lead_time-1]\n",
    "  sup_data = PC_norm[lead_time:]\n",
    "  print(sup_data.shape)\n",
    "  idx = np.where((rt.year <= 2015))[0]\n",
    "  sup_train = sup_data[idx]\n",
    "  idx = np.where((rt.year > 2015))[0]\n",
    "  sup_test = sup_data[idx]\n",
    "  print(sup_test.shape, sup_train.shape)\n",
    "  return data, rt, sup_train, sup_test, output_shape\n",
    "\n",
    "# 入力データの前処理\n",
    "def preprocess(data, rt, lead_time):\n",
    "  ipt_lag0  = data[10:-lead_time-1]\n",
    "  ipt_lag5  = data[5:-lead_time-6]\n",
    "  ipt_lag10 = data[:-lead_time-11]\n",
    "\n",
    "  # 検証データの作成\n",
    "  idx = np.where((rt.year > 2015))[0]\n",
    "  ipt_lag0_test = ipt_lag0[idx]\n",
    "  ipt_lag5_test = ipt_lag5[idx]\n",
    "  ipt_lag10_test = ipt_lag10[idx]\n",
    "  ipt_test = np.stack([ipt_lag0_test, ipt_lag5_test, ipt_lag10_test], 3)\n",
    "  return ipt_test\n",
    "\n",
    "# ==== iteration program ====\n",
    "lt_box = [0]\n",
    "#lt_box = [0, 5, 10, 15, 20, 25, 30, 35]\n",
    "#lt_box = np.arange(36)\n",
    "for lead_time in lt_box:\n",
    "\n",
    "  print('==== lead time : {} day ====='.format(lead_time))\n",
    "\n",
    "  data, rt, sup_train, sup_test, output_shape = indexing(lead_time) \n",
    "\n",
    "  olr_ipt_test = preprocess(olr_norm, rt, lead_time)\n",
    "  u850_ipt_test = preprocess(u850_norm, rt, lead_time)\n",
    "  v850_ipt_test = preprocess(v850_norm, rt, lead_time)\n",
    "  u200_ipt_test = preprocess(u200_norm, rt, lead_time)\n",
    "  v200_ipt_test = preprocess(v200_norm, rt, lead_time)\n",
    "  h850_ipt_test = preprocess(h850_norm, rt, lead_time)\n",
    "  pr_wtr_ipt_test = preprocess(pr_wtr_norm, rt, lead_time)\n",
    "  sst_ipt_test = preprocess(sst_norm, rt, lead_time)\n",
    "\n",
    "  ipt_test  = np.concatenate([olr_ipt_test, u850_ipt_test,  u200_ipt_test,\n",
    "                              v850_ipt_test, v200_ipt_test, h850_ipt_test, \n",
    "                              pr_wtr_ipt_test, sst_ipt_test], 3)\n",
    "  print(ipt_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error when deserializing class 'InputLayer' using config={'batch_shape': [None, 64, 64, 8], 'dtype': 'float32', 'sparse': False, 'name': 'input_layer'}.\n\nException encountered: Unrecognized keyword arguments: ['batch_shape']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m lead_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/work/gi55/i55233/data/machine_learning/results/model/model8_test030_wo-v850.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/keras/src/saving/saving_api.py:238\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    231\u001b[0m         filepath,\n\u001b[1;32m    232\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    234\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    235\u001b[0m     )\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/keras/src/engine/base_layer.py:870\u001b[0m, in \u001b[0;36mLayer.from_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 870\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when deserializing class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    873\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Error when deserializing class 'InputLayer' using config={'batch_shape': [None, 64, 64, 8], 'dtype': 'float32', 'sparse': False, 'name': 'input_layer'}.\n\nException encountered: Unrecognized keyword arguments: ['batch_shape']"
     ]
    }
   ],
   "source": [
    "# モデルの読み込み\n",
    "seed = 000\n",
    "lead_time = 0\n",
    "model_path = f'/work/gi55/i55233/data/machine_learning/results/model/model8_test030_wo-v850.h5'\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ipt_test[:300]\n",
    "shap.explainers._deep.deep_tf.op_handlers[\"FusedBatchNormV3\"] = shap.explainers._deep.deep_tf.passthrough # batch norm を挟む場合、このコードが必要：https://github.com/shap/shap/issues/1406\n",
    "explainer = shap.DeepExplainer(model = model, data = datasets)\n",
    "shap_values = explainer.shap_values(datasets, check_additivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
